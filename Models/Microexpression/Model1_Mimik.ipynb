{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow-Version:\", tf.__version__)\n",
    "\n",
    "# Liste der verfügbaren Geräte abrufen\n",
    "available_devices = tf.config.list_physical_devices()\n",
    "print(\"Verfügbare Geräte:\", available_devices)\n",
    "\n",
    "# Überprüfen, ob eine GPU verfügbar ist\n",
    "is_gpu_available = tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "print(\"Ist eine GPU verfügbar:\", is_gpu_available)\n",
    "\n",
    "# Spezifisch nach GPUs suchen\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs:\", gpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 8s 90ms/step - loss: 1.0416 - accuracy: 0.6856 - val_loss: 0.3216 - val_accuracy: 0.8922\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 6s 83ms/step - loss: 0.3061 - accuracy: 0.8909 - val_loss: 0.2129 - val_accuracy: 0.9182\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.1647 - accuracy: 0.9367 - val_loss: 0.1528 - val_accuracy: 0.9521\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.1505 - accuracy: 0.9452 - val_loss: 0.1124 - val_accuracy: 0.9621\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.0952 - accuracy: 0.9671 - val_loss: 0.1417 - val_accuracy: 0.9461\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.0895 - accuracy: 0.9696 - val_loss: 0.1144 - val_accuracy: 0.9581\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.0637 - accuracy: 0.9806 - val_loss: 0.0827 - val_accuracy: 0.9621\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.0747 - accuracy: 0.9751 - val_loss: 0.1059 - val_accuracy: 0.9641\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 0.0450 - accuracy: 0.9880 - val_loss: 0.0532 - val_accuracy: 0.9840\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.9783 - accuracy: 0.7588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9782769680023193, 0.7587859630584717]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "\n",
    "# TensorBoard Callback einrichten\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "\n",
    "# Kombinieren der ursprünglichen und augmentierten Daten\n",
    "train_ds = train_ds_original\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet101 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171446536/171446536 [==============================] - 14s 0us/step\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 18s 190ms/step - loss: 1.1826 - accuracy: 0.6667 - val_loss: 0.3472 - val_accuracy: 0.8862\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 11s 173ms/step - loss: 0.3078 - accuracy: 0.8949 - val_loss: 0.2340 - val_accuracy: 0.9122\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 11s 173ms/step - loss: 0.1891 - accuracy: 0.9317 - val_loss: 0.1861 - val_accuracy: 0.9281\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.1659 - accuracy: 0.9397 - val_loss: 0.1406 - val_accuracy: 0.9461\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 11s 173ms/step - loss: 0.1056 - accuracy: 0.9681 - val_loss: 0.1188 - val_accuracy: 0.9541\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 11s 172ms/step - loss: 0.1095 - accuracy: 0.9601 - val_loss: 0.0728 - val_accuracy: 0.9780\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.0736 - accuracy: 0.9791 - val_loss: 0.0663 - val_accuracy: 0.9780\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.0846 - accuracy: 0.9731 - val_loss: 0.0538 - val_accuracy: 0.9820\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.0411 - accuracy: 0.9890 - val_loss: 0.0446 - val_accuracy: 0.9840\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 2s 96ms/step - loss: 0.9238 - accuracy: 0.6821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9237698912620544, 0.6821086406707764]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet101  \n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet import preprocess_input  \n",
    "\n",
    "# TensorBoard Callback einrichten\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Kombinieren der ursprünglichen und augmentierten Daten\n",
    "train_ds = train_ds_original\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = ResNet101(input_shape=(224, 224, 3), include_top=False, weights='imagenet')  # Änderung zu ResNet101\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mit Dataaugmentation für ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 19989 files belonging to 4 classes.\n",
      "Using 15992 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/563 [==============================] - 115s 199ms/step - loss: 0.4564 - accuracy: 0.8351 - val_loss: 0.3219 - val_accuracy: 0.8583\n",
      "Epoch 2/10\n",
      "563/563 [==============================] - 287s 509ms/step - loss: 0.1793 - accuracy: 0.9341 - val_loss: 0.1936 - val_accuracy: 0.9301\n",
      "Epoch 3/10\n",
      "563/563 [==============================] - 272s 482ms/step - loss: 0.1405 - accuracy: 0.9473 - val_loss: 0.0425 - val_accuracy: 0.9840\n",
      "Epoch 4/10\n",
      "563/563 [==============================] - 274s 486ms/step - loss: 0.1051 - accuracy: 0.9600 - val_loss: 0.0341 - val_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "563/563 [==============================] - 295s 522ms/step - loss: 0.0979 - accuracy: 0.9653 - val_loss: 0.0235 - val_accuracy: 0.9920\n",
      "Epoch 6/10\n",
      "563/563 [==============================] - 300s 531ms/step - loss: 0.0841 - accuracy: 0.9679 - val_loss: 0.0667 - val_accuracy: 0.9721\n",
      "Epoch 7/10\n",
      "563/563 [==============================] - 195s 345ms/step - loss: 0.0711 - accuracy: 0.9741 - val_loss: 0.0383 - val_accuracy: 0.9880\n",
      "Epoch 8/10\n",
      "563/563 [==============================] - 219s 389ms/step - loss: 0.0628 - accuracy: 0.9776 - val_loss: 0.0183 - val_accuracy: 0.9960\n",
      "Epoch 9/10\n",
      "563/563 [==============================] - 235s 417ms/step - loss: 0.0675 - accuracy: 0.9767 - val_loss: 0.0204 - val_accuracy: 0.9940\n",
      "Epoch 10/10\n",
      "563/563 [==============================] - 217s 383ms/step - loss: 0.0457 - accuracy: 0.9837 - val_loss: 0.0447 - val_accuracy: 0.9800\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 14s 704ms/step - loss: 2.4736 - accuracy: 0.5974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4735658168792725, 0.5974441170692444]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Augmentierte Trainingsdaten laden und vorbereiten\n",
    "train_ds_augmented = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Dataaugmentation\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_augmented = preprocess_dataset(train_ds_augmented)\n",
    "\n",
    "# Kombinieren der ursprünglichen und augmentierten Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_augmented)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')  \n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=10, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mit Dataaugmentation für VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 19989 files belonging to 4 classes.\n",
      "Using 15992 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/4\n",
      "563/563 [==============================] - 433s 768ms/step - loss: 0.4220 - accuracy: 0.8499 - val_loss: 0.1473 - val_accuracy: 0.9321\n",
      "Epoch 2/4\n",
      "563/563 [==============================] - 355s 630ms/step - loss: 0.1475 - accuracy: 0.9447 - val_loss: 0.0813 - val_accuracy: 0.9721\n",
      "Epoch 3/4\n",
      "563/563 [==============================] - 378s 669ms/step - loss: 0.1106 - accuracy: 0.9575 - val_loss: 0.0709 - val_accuracy: 0.9800\n",
      "Epoch 4/4\n",
      "563/563 [==============================] - 426s 754ms/step - loss: 0.0851 - accuracy: 0.9682 - val_loss: 0.0528 - val_accuracy: 0.9820\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 32s 2s/step - loss: 3.7872 - accuracy: 0.6534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7871811389923096, 0.6533546447753906]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Augmentierte Trainingsdaten laden und vorbereiten\n",
    "train_ds_augmented = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Dataaugmentation\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_augmented = preprocess_dataset(train_ds_augmented)\n",
    "\n",
    "# Kombinieren der ursprünglichen und augmentierten Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_augmented)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')        # VGG19\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=4, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 + Dataaugmenation + FER-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 19989 files belonging to 4 classes.\n",
      "Using 15992 files for training.\n",
      "Found 16689 files belonging to 4 classes.\n",
      "Using 13352 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/4\n",
      "981/981 [==============================] - 165s 156ms/step - loss: 0.6774 - accuracy: 0.7386 - val_loss: 0.7373 - val_accuracy: 0.7665\n",
      "Epoch 2/4\n",
      "981/981 [==============================] - 146s 148ms/step - loss: 0.4682 - accuracy: 0.8089 - val_loss: 0.5983 - val_accuracy: 0.8802\n",
      "Epoch 3/4\n",
      "981/981 [==============================] - 148s 151ms/step - loss: 0.4101 - accuracy: 0.8323 - val_loss: 0.0536 - val_accuracy: 0.9721\n",
      "Epoch 4/4\n",
      "981/981 [==============================] - 149s 151ms/step - loss: 0.3638 - accuracy: 0.8502 - val_loss: 0.3349 - val_accuracy: 0.9022\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 7s 336ms/step - loss: 2.1834 - accuracy: 0.5415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1834187507629395, 0.5415335297584534]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten (ursprüngliches Dataset)\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Augmentierte Trainingsdaten laden und vorbereiten\n",
    "train_ds_augmented = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Dataaugmentation\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_augmented = preprocess_dataset(train_ds_augmented)\n",
    "\n",
    "# Zusätzliches Dataset laden und vorbereiten\n",
    "train_ds_additional = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\FER_Dataset\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_additional = preprocess_dataset(train_ds_additional)\n",
    "\n",
    "# Kombinieren der ursprünglichen, augmentierten und zusätzlichen Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_augmented).concatenate(train_ds_additional)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TensorBoard Callback vorbereiten\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=4, \n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 ohne Dautaaug mit FER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 16689 files belonging to 4 classes.\n",
      "Using 13352 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/4\n",
      "481/481 [==============================] - 72s 147ms/step - loss: 1.0075 - accuracy: 0.6121 - val_loss: 1.1057 - val_accuracy: 0.5150\n",
      "Epoch 2/4\n",
      "481/481 [==============================] - 73s 150ms/step - loss: 0.7685 - accuracy: 0.6804 - val_loss: 0.3427 - val_accuracy: 0.8703\n",
      "Epoch 3/4\n",
      "481/481 [==============================] - 73s 150ms/step - loss: 0.6722 - accuracy: 0.7229 - val_loss: 0.7506 - val_accuracy: 0.7924\n",
      "Epoch 4/4\n",
      "481/481 [==============================] - 73s 150ms/step - loss: 0.6159 - accuracy: 0.7463 - val_loss: 0.2279 - val_accuracy: 0.9162\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 4s 168ms/step - loss: 2.5355 - accuracy: 0.3850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5354554653167725, 0.38498401641845703]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten (ursprüngliches Dataset)\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "\n",
    "\n",
    "# Zusätzliches Dataset laden und vorbereiten\n",
    "train_ds_additional = image_dataset_from_directory(\n",
    "    r\"C:\\Thesis\\Data\\Frames\\Externe_Datasets\\AffecNet_Dataset\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_additional = preprocess_dataset(train_ds_additional)\n",
    "\n",
    "# Kombinieren der ursprünglichen, augmentierten und zusätzlichen Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_additional)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TensorBoard Callback vorbereiten\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=4, \n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 mit Dataugmenation mit Affecnet Dataset und Dropout + L1 Regulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 13788 files belonging to 4 classes.\n",
      "Using 11031 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/10\n",
      "408/408 [==============================] - 74s 176ms/step - loss: 3.3705 - accuracy: 0.5528 - val_loss: 3.1378 - val_accuracy: 0.3693\n",
      "Epoch 2/10\n",
      "408/408 [==============================] - 68s 166ms/step - loss: 1.4276 - accuracy: 0.5867 - val_loss: 2.0095 - val_accuracy: 0.3613\n",
      "Epoch 3/10\n",
      "408/408 [==============================] - 67s 164ms/step - loss: 1.1709 - accuracy: 0.5917 - val_loss: 2.2284 - val_accuracy: 0.3673\n",
      "Epoch 4/10\n",
      "408/408 [==============================] - 66s 158ms/step - loss: 1.1077 - accuracy: 0.5921 - val_loss: 2.3017 - val_accuracy: 0.3693\n",
      "Epoch 5/10\n",
      " 53/408 [==>...........................] - ETA: 1:02 - loss: 1.1355 - accuracy: 0.5831"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Modell trainieren\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Modell mit Testdaten testen\u001b[39;00m\n\u001b[0;32m     83\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mThesis\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFrames\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFacial_Expressions_Test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     85\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m     86\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1193\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1188\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1189\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1190\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1191\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1192\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1193\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1195\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Leonardo\\anaconda3\\envs\\Emotion_recognition\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input, VGG19\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Daten Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  RandomFlip(\"horizontal_and_vertical\"),\n",
    "  RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Zusätzliches Dataset laden und vorbereiten\n",
    "train_ds_additional = image_dataset_from_directory(\n",
    "    r\"C:\\Thesis\\Data\\Frames\\Externe_Datasets\\AffecNet_Dataset\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_additional = preprocess_dataset(train_ds_additional)\n",
    "\n",
    "# Kombinieren der ursprünglichen und zusätzlichen Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_additional)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)  # Daten Augmentation anwenden\n",
    "x = base_model(x, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)  # L2-Regularisierung\n",
    "x = Dropout(0.2)(x)  # Dropout hinzufügen\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TensorBoard Callback vorbereiten\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=10, \n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 ohne Dataaugmentation mit Affecnet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 13788 files belonging to 4 classes.\n",
      "Using 11031 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/20\n",
      "408/408 [==============================] - 66s 158ms/step - loss: 1.0392 - accuracy: 0.6078 - val_loss: 2.4046 - val_accuracy: 0.3912\n",
      "Epoch 2/20\n",
      "408/408 [==============================] - 62s 150ms/step - loss: 0.7344 - accuracy: 0.6871 - val_loss: 1.6384 - val_accuracy: 0.4431\n",
      "Epoch 3/20\n",
      "408/408 [==============================] - 62s 151ms/step - loss: 0.6208 - accuracy: 0.7345 - val_loss: 1.1719 - val_accuracy: 0.6048\n",
      "Epoch 4/20\n",
      "408/408 [==============================] - 62s 151ms/step - loss: 0.5541 - accuracy: 0.7698 - val_loss: 0.4260 - val_accuracy: 0.8244\n",
      "Epoch 5/20\n",
      "408/408 [==============================] - 62s 151ms/step - loss: 0.4835 - accuracy: 0.8000 - val_loss: 0.4661 - val_accuracy: 0.8323\n",
      "Epoch 6/20\n",
      "408/408 [==============================] - 64s 155ms/step - loss: 0.4136 - accuracy: 0.8325 - val_loss: 0.4598 - val_accuracy: 0.8263\n",
      "Epoch 7/20\n",
      "408/408 [==============================] - 63s 151ms/step - loss: 0.3712 - accuracy: 0.8508 - val_loss: 0.2377 - val_accuracy: 0.9142\n",
      "Epoch 8/20\n",
      "408/408 [==============================] - 72s 171ms/step - loss: 0.3155 - accuracy: 0.8756 - val_loss: 0.2984 - val_accuracy: 0.8902\n",
      "Epoch 9/20\n",
      "408/408 [==============================] - 70s 169ms/step - loss: 0.2689 - accuracy: 0.8984 - val_loss: 0.5882 - val_accuracy: 0.8563\n",
      "Epoch 10/20\n",
      "408/408 [==============================] - 68s 165ms/step - loss: 0.2527 - accuracy: 0.9039 - val_loss: 0.1803 - val_accuracy: 0.9501\n",
      "Epoch 11/20\n",
      "408/408 [==============================] - 66s 160ms/step - loss: 0.2259 - accuracy: 0.9139 - val_loss: 0.2097 - val_accuracy: 0.9341\n",
      "Epoch 12/20\n",
      "408/408 [==============================] - 65s 158ms/step - loss: 0.2005 - accuracy: 0.9267 - val_loss: 0.9638 - val_accuracy: 0.8244\n",
      "Epoch 13/20\n",
      "408/408 [==============================] - 66s 161ms/step - loss: 0.1752 - accuracy: 0.9383 - val_loss: 0.2544 - val_accuracy: 0.9242\n",
      "Epoch 14/20\n",
      "408/408 [==============================] - 68s 165ms/step - loss: 0.1741 - accuracy: 0.9361 - val_loss: 0.2205 - val_accuracy: 0.9242\n",
      "Epoch 15/20\n",
      "408/408 [==============================] - 78s 186ms/step - loss: 0.1412 - accuracy: 0.9497 - val_loss: 0.1660 - val_accuracy: 0.9561\n",
      "Epoch 16/20\n",
      "408/408 [==============================] - 67s 163ms/step - loss: 0.1382 - accuracy: 0.9518 - val_loss: 0.1785 - val_accuracy: 0.9581\n",
      "Epoch 17/20\n",
      "408/408 [==============================] - 68s 165ms/step - loss: 0.1207 - accuracy: 0.9580 - val_loss: 0.1616 - val_accuracy: 0.9541\n",
      "Epoch 18/20\n",
      "408/408 [==============================] - 68s 162ms/step - loss: 0.1269 - accuracy: 0.9571 - val_loss: 0.1320 - val_accuracy: 0.9661\n",
      "Epoch 19/20\n",
      "408/408 [==============================] - 68s 163ms/step - loss: 0.1225 - accuracy: 0.9570 - val_loss: 0.0939 - val_accuracy: 0.9721\n",
      "Epoch 20/20\n",
      "408/408 [==============================] - 67s 163ms/step - loss: 0.1144 - accuracy: 0.9590 - val_loss: 0.1452 - val_accuracy: 0.9501\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 3s 154ms/step - loss: 4.7708 - accuracy: 0.4665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.770761489868164, 0.4664536714553833]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten (ursprüngliches Dataset)\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "\n",
    "\n",
    "# Zusätzliches Dataset laden und vorbereiten\n",
    "train_ds_additional = image_dataset_from_directory(\n",
    "    r\"C:\\Thesis\\Data\\Frames\\Externe_Datasets\\AffecNet_Dataset\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "train_ds_additional = preprocess_dataset(train_ds_additional)\n",
    "\n",
    "# Kombinieren der ursprünglichen, augmentierten und zusätzlichen Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_additional)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TensorBoard Callback vorbereiten\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=20, \n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mit Dataaugmentation für DenseNet161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 19989 files belonging to 4 classes.\n",
      "Using 15992 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "51879936/51877672 [==============================] - 5s 0us/step\n",
      "Epoch 1/4\n",
      "563/563 [==============================] - 292s 502ms/step - loss: 0.4411 - accuracy: 0.8353 - val_loss: 0.2616 - val_accuracy: 0.8663\n",
      "Epoch 2/4\n",
      "563/563 [==============================] - 292s 518ms/step - loss: 0.1959 - accuracy: 0.9251 - val_loss: 0.3099 - val_accuracy: 0.8842\n",
      "Epoch 3/4\n",
      "563/563 [==============================] - 294s 521ms/step - loss: 0.1453 - accuracy: 0.9438 - val_loss: 0.0403 - val_accuracy: 0.9840\n",
      "Epoch 4/4\n",
      "563/563 [==============================] - 298s 529ms/step - loss: 0.1121 - accuracy: 0.9579 - val_loss: 0.0326 - val_accuracy: 0.9860\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 24s 1s/step - loss: 28.4996 - accuracy: 0.3610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[28.499591827392578, 0.36102235317230225]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Augmentierte Trainingsdaten laden und vorbereiten\n",
    "train_ds_augmented = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Dataaugmentation\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_augmented = preprocess_dataset(train_ds_augmented)\n",
    "\n",
    "# Kombinieren der ursprünglichen und augmentierten Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_augmented)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = DenseNet169(input_shape=(224, 224, 3), include_top=False, weights='imagenet')  # DenseNet 161\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=4, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mit Dataaugmentation für InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 19989 files belonging to 4 classes.\n",
      "Using 15992 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "563/563 [==============================] - 226s 395ms/step - loss: 0.5537 - accuracy: 0.7880 - val_loss: 0.2266 - val_accuracy: 0.9082\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 10s 476ms/step - loss: 140.4672 - accuracy: 0.2109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[140.4671630859375, 0.21086262166500092]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(299,299),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_original = preprocess_dataset(train_ds_original)\n",
    "\n",
    "# Augmentierte Trainingsdaten laden und vorbereiten\n",
    "train_ds_augmented = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Dataaugmentation\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(299, 299),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds_augmented = preprocess_dataset(train_ds_augmented)\n",
    "\n",
    "# Kombinieren der ursprünglichen und augmentierten Daten\n",
    "train_ds = train_ds_original.concatenate(train_ds_augmented)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(299, 299),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = InceptionV3(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=1, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(299, 299),\n",
    "    batch_size=32)\n",
    "model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
