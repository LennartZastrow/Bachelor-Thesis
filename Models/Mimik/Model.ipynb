{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models für Mimikanalyse : ResNet50 , Resnet101,  VGG16, VGG19, InceptionV3, DenseNet 169 und EfficientNetB7 ohne Dataaugmenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow-Version: 2.10.1\n",
      "Verfügbare Geräte: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From C:\\Users\\zastr\\AppData\\Local\\Temp\\ipykernel_5248\\3025205115.py:10: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Ist eine GPU verfügbar: True\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow-Version:\", tf.__version__)\n",
    "\n",
    "# Liste der verfügbaren Geräte abrufen\n",
    "available_devices = tf.config.list_physical_devices()\n",
    "print(\"Verfügbare Geräte:\", available_devices)\n",
    "\n",
    "# Überprüfen, ob eine GPU verfügbar ist\n",
    "is_gpu_available = tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "print(\"Ist eine GPU verfügbar:\", is_gpu_available)\n",
    "\n",
    "# Spezifisch nach GPUs suchen\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs:\", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard Callback einrichten\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 23s 195ms/step - loss: 1.0498 - accuracy: 0.6886 - val_loss: 0.3518 - val_accuracy: 0.9022\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 0.3020 - accuracy: 0.8924 - val_loss: 0.1893 - val_accuracy: 0.9361\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.1817 - accuracy: 0.9402 - val_loss: 0.1889 - val_accuracy: 0.9421\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.1509 - accuracy: 0.9532 - val_loss: 0.1171 - val_accuracy: 0.9661\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 20s 315ms/step - loss: 0.0981 - accuracy: 0.9681 - val_loss: 0.1487 - val_accuracy: 0.9421\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 12s 182ms/step - loss: 0.0923 - accuracy: 0.9696 - val_loss: 0.1574 - val_accuracy: 0.9301\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 10s 160ms/step - loss: 0.0855 - accuracy: 0.9731 - val_loss: 0.0729 - val_accuracy: 0.9741\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 11s 164ms/step - loss: 0.0686 - accuracy: 0.9781 - val_loss: 0.1114 - val_accuracy: 0.9601\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 0.0625 - val_accuracy: 0.9780\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 2s 61ms/step - loss: 0.0667 - accuracy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"ResNet50\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) # Dropout zur Regulierung von Overfitting\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 16s 198ms/step - loss: 0.9163 - accuracy: 0.7150 - val_loss: 0.2508 - val_accuracy: 0.9401\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 12s 177ms/step - loss: 0.2863 - accuracy: 0.8909 - val_loss: 0.2823 - val_accuracy: 0.9022\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 12s 183ms/step - loss: 0.1688 - accuracy: 0.9387 - val_loss: 0.1529 - val_accuracy: 0.9461\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 11s 169ms/step - loss: 0.1531 - accuracy: 0.9402 - val_loss: 0.0973 - val_accuracy: 0.9661\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 11s 172ms/step - loss: 0.0881 - accuracy: 0.9701 - val_loss: 0.0970 - val_accuracy: 0.9561\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 12s 180ms/step - loss: 0.0881 - accuracy: 0.9691 - val_loss: 0.1747 - val_accuracy: 0.9102\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 12s 177ms/step - loss: 0.0855 - accuracy: 0.9681 - val_loss: 0.0912 - val_accuracy: 0.9661\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 11s 164ms/step - loss: 0.0851 - accuracy: 0.9681 - val_loss: 0.0509 - val_accuracy: 0.9840\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 11s 165ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.1228 - val_accuracy: 0.9541\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.1500 - accuracy: 0.9521\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"ResNet101\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = ResNet101(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) # Dropout zur Regulierung von Overfitting\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 19s 230ms/step - loss: 0.7084 - accuracy: 0.7573 - val_loss: 0.1892 - val_accuracy: 0.9281\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 9s 133ms/step - loss: 0.2095 - accuracy: 0.9208 - val_loss: 0.1299 - val_accuracy: 0.9561\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 9s 134ms/step - loss: 0.1068 - accuracy: 0.9601 - val_loss: 0.1381 - val_accuracy: 0.9561\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 9s 137ms/step - loss: 0.1158 - accuracy: 0.9601 - val_loss: 0.0811 - val_accuracy: 0.9721\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 9s 134ms/step - loss: 0.0676 - accuracy: 0.9776 - val_loss: 0.0756 - val_accuracy: 0.9760\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 9s 134ms/step - loss: 0.0523 - accuracy: 0.9796 - val_loss: 0.0494 - val_accuracy: 0.9860\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 9s 133ms/step - loss: 0.0488 - accuracy: 0.9811 - val_loss: 0.0835 - val_accuracy: 0.9760\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 9s 138ms/step - loss: 0.0364 - accuracy: 0.9885 - val_loss: 0.0516 - val_accuracy: 0.9860\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 9s 138ms/step - loss: 0.0350 - accuracy: 0.9865 - val_loss: 0.0796 - val_accuracy: 0.9760\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 5s 219ms/step - loss: 0.0842 - accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"VGG16\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) # Dropout zur Regulierung von Overfitting\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 11s 171ms/step - loss: 0.7866 - accuracy: 0.7389 - val_loss: 0.2140 - val_accuracy: 0.9242\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 10s 156ms/step - loss: 0.1786 - accuracy: 0.9362 - val_loss: 0.2229 - val_accuracy: 0.9281\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.1220 - accuracy: 0.9591 - val_loss: 0.1787 - val_accuracy: 0.9401\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.1032 - accuracy: 0.9646 - val_loss: 0.1086 - val_accuracy: 0.9721\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 10s 156ms/step - loss: 0.0662 - accuracy: 0.9766 - val_loss: 0.1008 - val_accuracy: 0.9701\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.0597 - accuracy: 0.9801 - val_loss: 0.0984 - val_accuracy: 0.9741\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.0602 - accuracy: 0.9811 - val_loss: 0.0817 - val_accuracy: 0.9780\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.0402 - accuracy: 0.9880 - val_loss: 0.0629 - val_accuracy: 0.9860\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.0306 - accuracy: 0.9910 - val_loss: 0.0699 - val_accuracy: 0.9840\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 2s 108ms/step - loss: 0.0485 - accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"VGG19\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = VGG19(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) # Dropout zur Regulierung von Overfitting\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für die Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InceptionNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 25s 289ms/step - loss: 1.2272 - accuracy: 0.5790 - val_loss: 0.5846 - val_accuracy: 0.8224\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 14s 219ms/step - loss: 0.4772 - accuracy: 0.8371 - val_loss: 0.3114 - val_accuracy: 0.9202\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 15s 224ms/step - loss: 0.2797 - accuracy: 0.9228 - val_loss: 0.2737 - val_accuracy: 0.8922\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 14s 220ms/step - loss: 0.2388 - accuracy: 0.9248 - val_loss: 0.2201 - val_accuracy: 0.9202\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 14s 222ms/step - loss: 0.1524 - accuracy: 0.9532 - val_loss: 0.1649 - val_accuracy: 0.9441\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 14s 221ms/step - loss: 0.1346 - accuracy: 0.9581 - val_loss: 0.1212 - val_accuracy: 0.9621\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 14s 218ms/step - loss: 0.0901 - accuracy: 0.9771 - val_loss: 0.1043 - val_accuracy: 0.9681\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 14s 221ms/step - loss: 0.0818 - accuracy: 0.9761 - val_loss: 0.1215 - val_accuracy: 0.9601\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 19s 293ms/step - loss: 0.0667 - accuracy: 0.9821 - val_loss: 0.0744 - val_accuracy: 0.9760\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 5s 211ms/step - loss: 0.0969 - accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"InceptionV3\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(299,299),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(299, 299),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = InceptionV3(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(299, 299),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 28s 300ms/step - loss: 0.7953 - accuracy: 0.7120 - val_loss: 0.3377 - val_accuracy: 0.8583\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 16s 248ms/step - loss: 0.2972 - accuracy: 0.8869 - val_loss: 0.2201 - val_accuracy: 0.9441\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 16s 244ms/step - loss: 0.1636 - accuracy: 0.9497 - val_loss: 0.1676 - val_accuracy: 0.9401\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 17s 259ms/step - loss: 0.1359 - accuracy: 0.9547 - val_loss: 0.1435 - val_accuracy: 0.9541\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 17s 272ms/step - loss: 0.0895 - accuracy: 0.9751 - val_loss: 0.1124 - val_accuracy: 0.9661\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0830 - accuracy: 0.9751 - val_loss: 0.2322 - val_accuracy: 0.9062\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 15s 229ms/step - loss: 0.0669 - accuracy: 0.9781 - val_loss: 0.1057 - val_accuracy: 0.9581\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 25s 402ms/step - loss: 0.0508 - accuracy: 0.9846 - val_loss: 0.0754 - val_accuracy: 0.9840\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 0.0669 - val_accuracy: 0.9780\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.0735 - accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"DenseNet169\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "train_ds = preprocess_dataset(train_ds_original)\n",
    "\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = DenseNet169(input_shape=(224, 224, 3), include_top=False, weights='imagenet')  # DenseNet 161\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 files belonging to 4 classes.\n",
      "Using 2007 files for training.\n",
      "Found 2508 files belonging to 4 classes.\n",
      "Using 501 files for validation.\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.\n",
      "Epoch 1/9\n",
      "63/63 [==============================] - 39s 475ms/step - loss: 0.6362 - accuracy: 0.7514 - val_loss: 0.3139 - val_accuracy: 0.8583\n",
      "Epoch 2/9\n",
      "63/63 [==============================] - 29s 451ms/step - loss: 0.2631 - accuracy: 0.9018 - val_loss: 0.2437 - val_accuracy: 0.9082\n",
      "Epoch 3/9\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 0.1900 - accuracy: 0.9258 - val_loss: 0.1951 - val_accuracy: 0.9222\n",
      "Epoch 4/9\n",
      "63/63 [==============================] - 27s 429ms/step - loss: 0.1800 - accuracy: 0.9278 - val_loss: 0.1776 - val_accuracy: 0.9341\n",
      "Epoch 5/9\n",
      "63/63 [==============================] - 29s 452ms/step - loss: 0.1437 - accuracy: 0.9452 - val_loss: 0.0998 - val_accuracy: 0.9621\n",
      "Epoch 6/9\n",
      "63/63 [==============================] - 27s 432ms/step - loss: 0.1131 - accuracy: 0.9576 - val_loss: 0.0879 - val_accuracy: 0.9641\n",
      "Epoch 7/9\n",
      "63/63 [==============================] - 28s 433ms/step - loss: 0.0734 - accuracy: 0.9781 - val_loss: 0.0683 - val_accuracy: 0.9780\n",
      "Epoch 8/9\n",
      "63/63 [==============================] - 27s 432ms/step - loss: 0.0853 - accuracy: 0.9626 - val_loss: 0.0579 - val_accuracy: 0.9860\n",
      "Epoch 9/9\n",
      "63/63 [==============================] - 28s 431ms/step - loss: 0.0562 - accuracy: 0.9831 - val_loss: 0.0718 - val_accuracy: 0.9721\n",
      "Found 626 files belonging to 4 classes.\n",
      "20/20 [==============================] - 6s 250ms/step - loss: 0.1005 - accuracy: 0.9633\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import EfficientNetB7  # Update für EfficientNetB7\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input  # Update für EfficientNet-spezifische preprocess_input-Funktion\n",
    "\n",
    "# Betitelung für Tensorboard\n",
    "experiment_name = \"EfficientNetB7\"\n",
    "\n",
    "# Funktion zur Vorbereitung der Bilder\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Trainingsdaten laden und vorbereiten\n",
    "train_ds_original = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "# Preprocess\n",
    "train_ds = preprocess_dataset(train_ds_original)\n",
    "\n",
    "\n",
    "# Validierungsdaten laden und vorbereiten\n",
    "val_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "# Vortrainiertes Modell laden\n",
    "base_model = EfficientNetB7(input_shape=(224, 224, 3), include_top=False, weights='imagenet')  # Änderung zu EfficientNetB7\n",
    "base_model.trainable = False  # Einfrieren der Basis-Schichten\n",
    "\n",
    "# Modell anpassen\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)  # 4 Klassen für Ihre Emotionen\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren und trainieren\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setze den Pfad für die TensorBoard Logs\n",
    "log_dir = \"logs/fit/\" + experiment_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=\"epoch\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=9, \n",
    "    callbacks=[tensorboard_callback]  # TensorBoard Callback hinzufügen\n",
    ")\n",
    "\n",
    "# Modell mit Testdaten testen\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"C:\\\\Thesis\\\\Data\\\\Frames\\\\Facial_Expressions_Test\",\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "# Pfad für Test-Logs setzen\n",
    "test_log_dir = \"logs/fit/\" + experiment_name + \"_test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Summary Writer für Testdaten erstellen\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# Testmetriken in TensorBoard loggen\n",
    "with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('test_loss', test_loss, step=0)\n",
    "    tf.summary.scalar('test_accuracy', test_accuracy, step=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
